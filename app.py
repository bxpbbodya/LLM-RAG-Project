import re
import pickle
from pathlib import Path
from typing import List, Dict, Any, Tuple

import streamlit as st
from rank_bm25 import BM25Okapi

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.chat_models import ChatOllama

# --------------------------
# Paths
# --------------------------
INDEX_DIR = Path("index")
CHUNKS_STORE = INDEX_DIR / "chunks_store.pkl"

# --------------------------
# Text utilities
# --------------------------
def tokenize_ua(text: str) -> List[str]:
    return re.findall(r"[a-z–∞-—è—ñ—ó—î“ë0-9]+", text.lower())

def load_chunks() -> List[Dict[str, Any]]:
    with open(CHUNKS_STORE, "rb") as f:
        raw = pickle.load(f)

    chunks = []
    for ci in raw:
        text = getattr(ci, "text", None) or ci["text"]
        meta = getattr(ci, "metadata", None) or ci["metadata"]
        chunks.append({"text": text, "metadata": meta})
    return chunks

# --------------------------
# BM25
# --------------------------
def build_bm25(chunks):
    tokenized = [tokenize_ua(c["text"]) for c in chunks]
    return BM25Okapi(tokenized)

def bm25_retrieve(bm25, chunks, query, k):
    qtok = tokenize_ua(query)
    scores = bm25.get_scores(qtok)
    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
    return [chunks[i] for i in top if scores[i] > 0]

# --------------------------
# Dense
# --------------------------
def dense_retrieve(vs, query, k):
    docs = vs.similarity_search(query, k=k)
    return [{"text": d.page_content, "metadata": d.metadata} for d in docs]

# --------------------------
# Hybrid
# --------------------------
def hybrid_retrieve(bm25, vs, chunks, query, k):
    bm = bm25_retrieve(bm25, chunks, query, k)
    dn = dense_retrieve(vs, query, k)

    merged = {}
    for c in bm + dn:
        key = (c["metadata"].get("source"), c["metadata"].get("page"), c["text"][:80])
        merged[key] = c

    return list(merged.values())[:k]

# --------------------------
# Prompt
# --------------------------
def build_prompt(question, chunks):
    ctx = []
    for i, c in enumerate(chunks, start=1):
        m = c["metadata"]
        ctx.append(f"[{i}] {m.get('source','?')} {c['text']}")
    context = "\n\n".join(ctx)

    return f"""
–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.
–°—Ç–∞–≤ –ø–æ—Å–∏–ª–∞–Ω–Ω—è [1], [2] —É —Ç–µ–∫—Å—Ç—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.
–Ø–∫—â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–µ–º–∞—î ‚Äî –Ω–∞–ø–∏—à–∏: "–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö —É –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

–ü–∏—Ç–∞–Ω–Ω—è: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}
""".strip()

# --------------------------
# UI
# --------------------------
def main():
    st.set_page_config(page_title="RAG QA (Ollama)", layout="wide")
    st.title("üìö RAG Question Answering (–ª–æ–∫–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ Ollama)")
    st.info("UI –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–≤—Å—è ‚úÖ –Ø–∫—â–æ —â–æ—Å—å –ø—ñ–¥–µ –Ω–µ —Ç–∞–∫ ‚Äî –ø–æ–º–∏–ª–∫—É –±—É–¥–µ –ø–æ–∫–∞–∑–∞–Ω–æ —Ç—É—Ç")

    if not CHUNKS_STORE.exists():
        st.error("‚ùå –ù–µ–º–∞—î index/chunks_store.pkl ‚Üí –∑–∞–ø—É—Å—Ç–∏ python ingest.py")
        st.stop()

    chunks = load_chunks()

    # Sidebar
    st.sidebar.header("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è")
    mode = st.sidebar.radio(
        "Retriever",
        ["Hybrid", "BM25", "Dense", "–ë–µ–∑ –ø–æ—à—É–∫—É"],
        index=0
    )
    k = st.sidebar.slider("k (—á–∞–Ω–∫–∏)", 2, 10, 5)
    temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.2)

    # Init components (SAFE)
    try:
        with st.spinner("–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π‚Ä¶"):
            embeddings = OllamaEmbeddings(model="nomic-embed-text")
            vectorstore = FAISS.load_local(
                str(INDEX_DIR),
                embeddings,
                allow_dangerous_deserialization=True
            )
            bm25 = build_bm25(chunks)
            llm = ChatOllama(model="mistral", temperature=temperature)
        st.success("–ì–æ—Ç–æ–≤–æ ‚úÖ")
    except Exception as e:
        st.error("‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó (Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞?)")
        st.exception(e)
        st.stop()

    question = st.text_input(
        "–ü–∏—Ç–∞–Ω–Ω—è",
        placeholder="–ù–∞–ø—Ä.: –©–æ —Ä–æ–±–∏—Ç–∏, —è–∫—â–æ –≤—ñ–¥—á—É–≤–∞—î—Ç—å—Å—è –∑–∞–ø–∞—Ö –≥–∞—Ä—É?"
    )

    if st.button("üîé –ó–∞–ø–∏—Ç–∞—Ç–∏") and question.strip():
        with st.spinner("–ü–æ—à—É–∫ —ñ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ‚Ä¶"):
            if mode == "BM25":
                retrieved = bm25_retrieve(bm25, chunks, question, k)
            elif mode == "Dense":
                retrieved = dense_retrieve(vectorstore, question, k)
            elif mode == "Hybrid":
                retrieved = hybrid_retrieve(bm25, vectorstore, chunks, question, k)
            else:
                retrieved = []

            prompt = build_prompt(question, retrieved) if retrieved else question
            answer = llm.invoke(prompt).content

        st.subheader("–í—ñ–¥–ø–æ–≤—ñ–¥—å")
        st.write(answer)

        if retrieved:
            st.subheader("–î–∂–µ—Ä–µ–ª–∞")
            for i, c in enumerate(retrieved, start=1):
                m = c["metadata"]
                st.markdown(f"**[{i}] {m.get('source','?')}**")
                st.code(c["text"][:800], language="text")

# --------------------------
# ENTRY POINT
# --------------------------
if __name__ == "__main__":
    main()
